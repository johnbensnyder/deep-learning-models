{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from time import time\n",
    "# temp\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append('/workspace/shared_workspace/deep-learning-models/models/vision/detection')\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow as hvd\n",
    "\n",
    "from awsdet.models.detectors.mask_rcnn import MaskRCNN\n",
    "from awsdet.datasets import DATASETS, build_dataloader\n",
    "from awsdet.datasets import build_dataset, build_dataloader\n",
    "from awsdet.models import build_detector\n",
    "from awsdet.utils.schedulers import schedulers\n",
    "from awsdet.core import CocoDistEvalmAPHook, CocoDistEvalRecallHook\n",
    "from awsdet.utils.runner.hooks.logger import tensorboard, text\n",
    "from awsdet.utils.runner.hooks import checkpoint, iter_timer, visualizer\n",
    "from awsdet.apis.train import parse_losses, batch_processor, build_optimizer, get_root_logger\n",
    "from awsdet.utils.misc import Config\n",
    "import horovod.tensorflow as hvd\n",
    "from awsdet.utils.runner import sagemaker_runner\n",
    "from awsdet.utils.schedulers.schedulers import WarmupScheduler\n",
    "import argparse\n",
    "\n",
    "from awsdet.utils import visualize\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "from awsdet.core.bbox import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from awsdet.utils.fileio import load, dump\n",
    "import os.path as osp\n",
    "from awsdet.core.evaluation.coco_utils import fast_eval_recall, results2json\n",
    "from awsdet.core.mask.transforms import mask2result\n",
    "from awsdet.core.evaluation import coco_utils\n",
    "##########################################################################################\n",
    "# Setup horovod and tensorflow environment\n",
    "##########################################################################################\n",
    "\n",
    "fp16 = True\n",
    "hvd.init()\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": fp16})\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile(\"/workspace/shared_workspace/deep-learning-models/models/vision/detection/configs/docker_mask_rcnn.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.36s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cfg.batch_size_per_device = 2\n",
    "cfg.workers_per_gpu = 1\n",
    "cfg.global_batch_size = cfg.batch_size_per_device * hvd.size()\n",
    "datasets = build_dataset(cfg.data.train)\n",
    "tf_datasets = [build_dataloader(datasets,\n",
    "                     cfg.batch_size_per_device,\n",
    "                     cfg.workers_per_gpu,\n",
    "                     num_gpus=hvd.size(),\n",
    "                     dist=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.46s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "val_dataset = build_dataset(cfg.data.val)\n",
    "val_tdf, val_size = build_dataloader(val_dataset,\n",
    "                                     cfg.batch_size_per_device,\n",
    "                                     cfg.workers_per_gpu,\n",
    "                                     num_gpus=hvd.size(),\n",
    "                                     dist=True,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_detector(cfg.model,\n",
    "                           train_cfg=cfg.train_cfg,\n",
    "                           test_cfg=cfg.test_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new loop for GPU: 0\n"
     ]
    }
   ],
   "source": [
    "_ = model(next(iter(tf_datasets[0][0])), training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.base_learning_rate = 0.01\n",
    "cfg.warmup_init_lr_scale = 3.0\n",
    "cfg.warmup_steps = 500\n",
    "base_learning_rate = cfg.base_learning_rate\n",
    "scaled_learning_rate = base_learning_rate * cfg.global_batch_size / 8\n",
    "steps_per_epoch = 100\n",
    "scheduler = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "            [steps_per_epoch * 8, steps_per_epoch * 10],\n",
    "            [scaled_learning_rate, scaled_learning_rate*0.1, scaled_learning_rate*0.01])\n",
    "warmup_init_lr = 1.0 / cfg.warmup_init_lr_scale * scaled_learning_rate\n",
    "scheduler = WarmupScheduler(scheduler, warmup_init_lr, cfg.warmup_steps)\n",
    "optimizer = tf.keras.optimizers.SGD(scheduler, momentum=0.9, nesterov=False)\n",
    "optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer, loss_scale='dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "runner = sagemaker_runner.Runner(model, batch_processor, name='mask_test', \n",
    "                                     optimizer=optimizer, work_dir='/workspace/shared_workspace/logs',\n",
    "                                     logger=get_root_logger(cfg.log_level), amp_enabled=True,\n",
    "                                     loss_weights=cfg.loss_weights, with_mask=True)\n",
    "runner.register_hook(iter_timer.IterTimerHook())\n",
    "runner.register_hook(text.TextLoggerHook())\n",
    "runner.register_hook(CocoDistEvalmAPHook(cfg.data.val, interval=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-21 18:22:25,799 - INFO - Loading checkpoint from /workspace/shared_workspace/output_spike_2/007/mask_rcnn...\n",
      "2020-06-21 18:22:26,840 - INFO - Loaded weights from checkpoint: /workspace/shared_workspace/output_spike_2/007/mask_rcnn\n"
     ]
    }
   ],
   "source": [
    "runner.load_checkpoint('/workspace/shared_workspace/output_spike_2/007/mask_rcnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<awsdet.models.necks.fpn.FPN at 0x7f3a84882090>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runner.run(tf_datasets, cfg.workflow, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217c7c1522284487af782bba5b81eecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new loop for GPU: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps = 10\n",
    "results = []\n",
    "masks = []\n",
    "for i, batch in tqdm(enumerate(val_tdf.take(steps)), total=steps):\n",
    "    img, _ = batch\n",
    "    detection = runner.model(batch, training=False)\n",
    "    bboxes = transforms.bbox_mapping_back(detection['bboxes'], batch[1][0])\n",
    "    box_ints = tf.round(bboxes)\n",
    "    labels = detection['labels']\n",
    "    scores = detection['scores']\n",
    "    result = transforms.bbox2result(bboxes, labels, \n",
    "                                          scores, num_classes=val_dataset.CLASSES+1)\n",
    "    #masks = mask2result(bboxes, detection['masks'], labels, batch[1][0])\n",
    "    results.append(result)\n",
    "    masks.append(detection['masks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "def box_clip(bboxes, img_size):\n",
    "    y1, x1, y2, x2 = np.split(bboxes, 4, axis=1)\n",
    "    cy1 = np.clip(y1, 0, img_size[0])\n",
    "    cx1 = np.clip(x1, 0, img_size[1])\n",
    "    cy2 = np.clip(y2, 0, img_size[0])\n",
    "    cx2 = np.clip(x2, 0, img_size[1])\n",
    "    clipped_boxes = np.transpose(np.squeeze(np.stack([cy1, cx1, cy2, cx2])))\n",
    "    '''cheight = cy2 - cy1\n",
    "    cwidth = cx2 - cx1\n",
    "    crop_height_start = np.abs(y1)\n",
    "    crop_width_start = np.abs(x1)'''\n",
    "    return clipped_boxes\n",
    "\n",
    "def compute_pads(bboxes, img_size):\n",
    "    y1, x1, y2, x2 = np.split(bboxes, 4, axis=1)\n",
    "    y1_pad = y1\n",
    "    x1_pad = x1\n",
    "    y2_pad = img_size[0] - y2\n",
    "    x2_pad = img_size[1] - x2\n",
    "    pads = np.transpose(np.squeeze(np.stack([y1_pad, x1_pad, y2_pad, x2_pad])))\n",
    "    return pads\n",
    "\n",
    "def fit_pad(mask, pad):\n",
    "    mask = np.pad(mask, ((pad[0], pad[2]), (pad[1], pad[3])))\n",
    "    return mask\n",
    "\n",
    "def fit_mask_to_image(mask, cv2_size, clipped_box_size, \n",
    "                      clipped_boxes_np, pad, img_shape):\n",
    "    if np.multiply(*clipped_box_size)==0:\n",
    "        return np.zeros(img_shape, dtype=np.int32)\n",
    "    mask = cv2.resize(mask, tuple(cv2_size), interpolation=cv2.INTER_NEAREST)\n",
    "    if mask.ndim==1:\n",
    "        mask = np.expand_dims(mask, axis=np.argmin(clipped_box_size))\n",
    "    mask = mask[:clipped_box_size[0],:clipped_box_size[1]]\n",
    "    #print('\\n')\n",
    "    #print(mask.shape)\n",
    "    mask = fit_pad(mask, pad)\n",
    "    #print(mask.shape)\n",
    "    #print(clipped_boxes_np)\n",
    "    return mask\n",
    "\n",
    "def reshape_by_labels(mask_list, labels, num_classes=81):\n",
    "    list_of_lists = [[]]*num_classes\n",
    "    for mask, label in zip(mask_list, labels):\n",
    "        list_of_lists[label].append(mask)\n",
    "    return list_of_lists\n",
    "\n",
    "def mask2result(bboxes, masks, labels, meta, num_classes=81, threshold=0.5):\n",
    "    # convert tensors to numpy\n",
    "    # round bboxes to nearest int\n",
    "    meta = np.squeeze(meta)\n",
    "    img_heights, img_widths = meta[:2].astype(np.int32)\n",
    "    bboxes_np = np.round(bboxes).astype(np.int32)\n",
    "    clipped_boxes_np = box_clip(bboxes_np, (img_heights, img_widths))\n",
    "    masks_np = (masks.numpy()>threshold).astype(np.int32)\n",
    "    labels_np = labels.numpy()\n",
    "    if meta[-1]==1:\n",
    "        masks_np = np.flip(masks_np, axis=2)\n",
    "    bbox_heights = bboxes_np[:,2]-bboxes_np[:,0]\n",
    "    bbox_clipped_heights = clipped_boxes_np[:,2]-clipped_boxes_np[:,0]\n",
    "    bbox_widths = bboxes_np[:,3]-bboxes_np[:,1]\n",
    "    bbox_clipped_widths = clipped_boxes_np[:,3]-clipped_boxes_np[:,1]\n",
    "    bbox_sizes = np.transpose(np.stack([bbox_heights, bbox_widths]))\n",
    "    #cv2 needs dims in opposite direction\n",
    "    cv2_sizes = np.flip(bbox_sizes, axis=1)\n",
    "    bbox_clipped_sizes = np.squeeze(np.transpose([np.stack([bbox_clipped_heights,\n",
    "                                                 bbox_clipped_widths])]))\n",
    "    pads = compute_pads(clipped_boxes_np, (img_heights, img_widths))\n",
    "    \n",
    "    mask_list = []\n",
    "    for idx in range(100):\n",
    "        #print(idx)\n",
    "        mask_list.append(fit_mask_to_image(masks_np[idx],\n",
    "                                           cv2_sizes[idx],\n",
    "                                           bbox_clipped_sizes[idx],\n",
    "                                           clipped_boxes_np[idx],\n",
    "                                           pads[idx],\n",
    "                                           (img_heights, img_widths)))\n",
    "    lists = defaultdict(list)\n",
    "    for i,j in enumerate(labels_np):\n",
    "        lists[j].append(mask_util.encode(\n",
    "                    np.array(\n",
    "                        mask_list[i][:, :, np.newaxis], order='F',\n",
    "                        dtype='uint8'))[0])\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.8 ms ± 153 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mask_list = \\\n",
    "mask2result(bboxes, detection['masks'], labels, batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230054b00f08464181fd4b9747b64371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new loop for GPU: 0\n"
     ]
    }
   ],
   "source": [
    "steps = 500\n",
    "masks_list = []\n",
    "result_list = []\n",
    "for i, batch in tqdm(enumerate(val_tdf.take(steps)), total=steps):\n",
    "    detection = runner.model(batch, training=False)\n",
    "    bboxes = transforms.bbox_mapping_back(detection['bboxes'], batch[1][0])\n",
    "    box_ints = tf.round(bboxes)\n",
    "    labels = detection['labels']\n",
    "    scores = detection['scores']\n",
    "    result = transforms.bbox2result(bboxes, labels, \n",
    "                                          scores, num_classes=val_dataset.CLASSES+1)\n",
    "    masks = mask2result(box_ints, detection['masks'], labels, batch[1][0])\n",
    "    #masks_list.append(masks)\n",
    "    result_list.append((result, masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "coco_utils = reload(coco_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = coco_utils.results2json(val_dataset, results, 'temp_eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bbox': 'temp_eval.bbox.json', 'segm': 'temp_eval.segm.json'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cb89a8843901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetection\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox_mapping_back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bboxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#bboxes_round = tf.round(transforms.bbox_mapping_back(detections['bboxes'], batch[1][0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "for detection in detections:\n",
    "    bboxes = tf.round(transforms.bbox_mapping_back(detections['bboxes'], batch[1][0]))\n",
    "    #bboxes_round = tf.round(transforms.bbox_mapping_back(detections['bboxes'], batch[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<awsdet.models.detectors.mask_rcnn.MaskRCNN at 0x7f62cb884610>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
