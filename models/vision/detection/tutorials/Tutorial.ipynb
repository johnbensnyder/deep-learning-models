{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWSDet Sagemaker Tutorial\n",
    "\n",
    "This tutorial walks through the entire process of training an object detection model on Sagemaker. The tutorial does not go into the details of the model itself. Rather, we focus only on setting up and training the model. For a detailed explanation of individual models, please look at resources mentioned in the README.md for that model.\n",
    "\n",
    "We assume that you are running this notebook on a SageMaker Notebook instance. For instructions on how to setup a SageMaker Notebook instance for this tutorial, please see [README.md](./README.md)\n",
    "\n",
    "The following are the major components that are required,\n",
    "- Data download and preparation\n",
    "- Docker container\n",
    "- Hyperparameter settings\n",
    "- Launching a model\n",
    "- Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download and preparation\n",
    "The `prep_data_s3.sh` shell script contains everything to download the COCO 2017 dataset, as well as resnet weights pretrained on imagenet data. These are the same weights included in the Keras package. We download them manually for stability, because they are sometimes retrained between versions of Keras. The script will download everything to the Sagemaker notebook instance, assemble everything in the file structure the training script expects to see, archives it, and saves it to your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/SageMaker\n",
    "echo \"Current dir: $PWD\"\n",
    "# IMPORTANT if deep-learning-models directory already exists then backup (optional) and remove it so that git clone can succeed\n",
    "git clone https://github.com/aws-samples/deep-learning-models\n",
    "cd deep-learning-models/models/vision/detection/\n",
    "# IMPORTANT replace argument to the script with name of your s3 bucket not including s3://\n",
    "scripts/prep_data_s3.sh mzanur-sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker container\n",
    "\n",
    "While the data preparation script mentioned above is running, you can prepare a Docker image for SageMaker. When you launch a Sagemaker training job, each instance in that job will download a Docker image from Elastic Container Registry (ECR), setup a series of environment variables so it knows your hyperparameters and where to find the data, and launches your training script. Most of the time, you can use Sagemaker's built in containers, but for customized models you could have to create your own.\n",
    "\n",
    "Under `deep-learning-models/models/vision/detection/docker` you'll find the docker file `Dockerfile.sagemaker` that creates the image that we use for training. A detailed description of that file, and how to create your own, is included in  [README.md](../docker/README.md). Below are the basic commands to build this image on your SageMaker notebook instance and upload it to ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  528.9kB\n",
      "Step 1/43 : FROM nvidia/cuda:10.1-base-ubuntu18.04\n",
      " ---> 3b55548ae91f\n",
      "Step 2/43 : LABEL maintainer=\"Amazon AI\"\n",
      " ---> Using cache\n",
      " ---> c56dde051dc6\n",
      "Step 3/43 : ENV DEBIAN_FRONTEND noninteractive\n",
      " ---> Using cache\n",
      " ---> 5dffef1a4cc4\n",
      "Step 4/43 : ENV DEBCONF_NONINTERACTIVE_SEEN true\n",
      " ---> Using cache\n",
      " ---> 7ba83002e360\n",
      "Step 5/43 : ENV SAGEMAKER_TRAINING_MODULE sagemaker_tensorflow_container.training:main\n",
      " ---> Using cache\n",
      " ---> 09c2a9f02431\n",
      "Step 6/43 : ENV PYTHONDONTWRITEBYTECODE=1\n",
      " ---> Using cache\n",
      " ---> 1b082325b61b\n",
      "Step 7/43 : ENV PYTHONUNBUFFERED=1\n",
      " ---> Using cache\n",
      " ---> da3b14f5c907\n",
      "Step 8/43 : ENV PYTHONIOENCODING=UTF-8\n",
      " ---> Using cache\n",
      " ---> 297ef8077a08\n",
      "Step 9/43 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 28150667cf22\n",
      "Step 10/43 : ENV LC_ALL=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 9febde0a5c70\n",
      "Step 11/43 : ENV KMP_AFFINITY=granularity=fine,compact,1,0\n",
      " ---> Using cache\n",
      " ---> ebc26e1cfe08\n",
      "Step 12/43 : ENV KMP_BLOCKTIME=1\n",
      " ---> Using cache\n",
      " ---> 72cf29172d94\n",
      "Step 13/43 : ENV KMP_SETTINGS=0\n",
      " ---> Using cache\n",
      " ---> d3f44c3c814d\n",
      "Step 14/43 : ARG PYTHON=python3\n",
      " ---> Using cache\n",
      " ---> 0794cffd73e7\n",
      "Step 15/43 : ARG PYTHON_PIP=python3-pip\n",
      " ---> Using cache\n",
      " ---> b3ed8cea01b1\n",
      "Step 16/43 : ARG PIP=pip3\n",
      " ---> Using cache\n",
      " ---> a161c3cdf6ee\n",
      "Step 17/43 : ARG TF_URL=https://tensorflow-aws.s3-us-west-2.amazonaws.com/2.1/AmazonLinux/gpu/final/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl\n",
      " ---> Using cache\n",
      " ---> 9a0a78e5ebb2\n",
      "Step 18/43 : RUN apt-get update && apt-get install -y --no-install-recommends --allow-unauthenticated     python3-dev     python3-pip     python3-setuptools     ca-certificates     cuda-command-line-tools-10-1     cuda-cudart-dev-10-1     cuda-cufft-dev-10-1     cuda-curand-dev-10-1     cuda-cusolver-dev-10-1     cuda-cusparse-dev-10-1     curl     libcudnn7=7.6.2.24-1+cuda10.1     libnccl2=2.4.7-1+cuda10.1     libgomp1     libnccl-dev=2.4.7-1+cuda10.1     libfreetype6-dev     libhdf5-serial-dev     libpng-dev     libzmq3-dev     git     wget     vim     build-essential     openssh-client     openssh-server     zlib1g-dev     libgtk2.0-dev     && apt-get update     && apt-get install -y --no-install-recommends --allow-unauthenticated --allow-downgrades     libcublas10=10.1.0.105-1     libcublas-dev=10.1.0.105-1  && apt-get update && apt-get install -y --no-install-recommends --allow-unauthenticated      nvinfer-runtime-trt-repo-ubuntu1804-5.0.2-ga-cuda10.0  && apt-get update && apt-get install -y --no-install-recommends --allow-unauthenticated      libnvinfer6=6.0.1-1+cuda10.1  && rm -rf /var/lib/apt/lists/*  && mkdir -p /var/run/sshd\n",
      " ---> Using cache\n",
      " ---> 6260e7ca89c7\n",
      "Step 19/43 : RUN mkdir /tmp/openmpi  && cd /tmp/openmpi  && curl -fSsL -O https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.1.tar.gz  && tar zxf openmpi-4.0.1.tar.gz  && cd openmpi-4.0.1  && ./configure --enable-orterun-prefix-by-default  && make -j $(nproc) all  && make install  && ldconfig  && rm -rf /tmp/openmpi\n",
      " ---> Using cache\n",
      " ---> 72ce23f2f0da\n",
      "Step 20/43 : RUN mv /usr/local/bin/mpirun /usr/local/bin/mpirun.real  && echo '#!/bin/bash' > /usr/local/bin/mpirun  && echo 'mpirun.real --allow-run-as-root \"$@\"' >> /usr/local/bin/mpirun  && chmod a+x /usr/local/bin/mpirun\n",
      " ---> Using cache\n",
      " ---> 1b7430204cad\n",
      "Step 21/43 : RUN echo \"hwloc_base_binding_policy = none\" >> /usr/local/etc/openmpi-mca-params.conf  && echo \"rmaps_base_mapping_policy = slot\" >> /usr/local/etc/openmpi-mca-params.conf\n",
      " ---> Using cache\n",
      " ---> d2dfd90c595f\n",
      "Step 22/43 : RUN echo NCCL_DEBUG=INFO >> /etc/nccl.conf\n",
      " ---> Using cache\n",
      " ---> cba6e81dccbc\n",
      "Step 23/43 : ENV LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/openmpi/lib:$LD_LIBRARY_PATH\n",
      " ---> Using cache\n",
      " ---> 546fd350c0f3\n",
      "Step 24/43 : ENV PATH /usr/local/openmpi/bin/:$PATH\n",
      " ---> Using cache\n",
      " ---> 92dde667c50f\n",
      "Step 25/43 : ENV PATH=/usr/local/nvidia/bin:$PATH\n",
      " ---> Using cache\n",
      " ---> e2bd4519f106\n",
      "Step 26/43 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 3c575d36ad40\n",
      "Step 27/43 : RUN ${PIP} --no-cache-dir install --upgrade     pip     setuptools\n",
      " ---> Using cache\n",
      " ---> 45bbc0bb87e8\n",
      "Step 28/43 : RUN ln -s $(which ${PYTHON}) /usr/local/bin/python  && ln -s $(which ${PIP}) /usr/bin/pip\n",
      " ---> Using cache\n",
      " ---> 9c1aba72f005\n",
      "Step 29/43 : RUN ${PIP} install --no-cache-dir -U     numpy==1.17.5     scipy==1.2.2     scikit-learn==0.22     pandas==1.0.1     Pillow==7.0.0     h5py==2.10.0     keras_applications==1.0.8     keras_preprocessing==1.1.0     keras==2.3.1     smdebug==0.7.1     python-dateutil==2.8.1     pyYAML==5.3.1     requests==2.22.0     awscli     mpi4py==3.0.3     opencv-python==4.2.0.32     sagemaker==1.50.17     sagemaker-experiments==0.1.7     \"sagemaker-tensorflow>=2.1,<2.2\"     \"sagemaker-tensorflow-training>2,<4\"  && ${PIP} install --no-cache-dir -U     ${TF_URL}\n",
      " ---> Using cache\n",
      " ---> 2a4a61e11f72\n",
      "Step 30/43 : RUN ldconfig /usr/local/cuda-10.1/targets/x86_64-linux/lib/stubs  && HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_WITH_TENSORFLOW=1 ${PIP} install --no-cache-dir horovod==0.18.2  && ldconfig\n",
      " ---> Using cache\n",
      " ---> e41b0ebb22f6\n",
      "Step 31/43 : RUN mkdir -p /var/run/sshd  && sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd\n",
      " ---> Using cache\n",
      " ---> 04896091cd3c\n",
      "Step 32/43 : RUN mkdir -p /root/.ssh/  && ssh-keygen -q -t rsa -N '' -f /root/.ssh/id_rsa  && cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys  && printf \"Host *\\n  StrictHostKeyChecking no\\n\" >> /root/.ssh/config\n",
      " ---> Using cache\n",
      " ---> 4afe025d171f\n",
      "Step 33/43 : RUN cat /etc/ssh/ssh_config | grep -v StrictHostKeyChecking > /etc/ssh/ssh_config.new  && echo \"    StrictHostKeyChecking no\" >> /etc/ssh/ssh_config.new  && mv /etc/ssh/ssh_config.new /etc/ssh/ssh_config\n",
      " ---> Using cache\n",
      " ---> 0dbe37e4f26c\n",
      "Step 34/43 : ADD https://raw.githubusercontent.com/aws/aws-deep-learning-containers-utils/master/deep_learning_container.py  /usr/local/bin/deep_learning_container.py\n",
      "\n",
      " ---> Using cache\n",
      " ---> b1ca130ed0df\n",
      "Step 35/43 : RUN chmod +x /usr/local/bin/deep_learning_container.py\n",
      " ---> Using cache\n",
      " ---> 777aaf9fdf2e\n",
      "Step 36/43 : RUN curl https://aws-dlc-licenses.s3.amazonaws.com/tensorflow-2.1/license.txt -o /license.txt\n",
      " ---> Using cache\n",
      " ---> 3b8fc6926347\n",
      "Step 37/43 : ARG DALI_VERSION=0.20.0\n",
      " ---> Using cache\n",
      " ---> 1c8d7d775f63\n",
      "Step 38/43 : RUN ${PIP} install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/10.0             nvidia-dali==$DALI_VERSION &&     ${PIP} install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/10.0             nvidia-dali-tf-plugin==$DALI_VERSION\n",
      " ---> Using cache\n",
      " ---> e3991a495d84\n",
      "Step 39/43 : ENV TZ=America/Los_Angeles\n",
      " ---> Using cache\n",
      " ---> 1952e49c42e0\n",
      "Step 40/43 : RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n",
      " ---> Using cache\n",
      " ---> a97887573c7f\n",
      "Step 41/43 : RUN apt update && apt install -y python3-opencv libopenblas-base         libomp-dev build-essential\n",
      " ---> Using cache\n",
      " ---> f8db697d9df1\n",
      "Step 42/43 : RUN ${PIP} install tensorflow_addons==0.8.3                 s3fs                 ipykernel                 imgaug                 tqdm                 tensorflow_datasets==2.1.0                 scikit-image                 cython                 addict                 terminaltables                 numba &&     pip install pycocotools\n",
      " ---> Using cache\n",
      " ---> 955c0f5c2a0b\n",
      "Step 43/43 : CMD [\"bin/bash\"]\n",
      " ---> Using cache\n",
      " ---> 240ce6661ab0\n",
      "Successfully built 240ce6661ab0\n",
      "Successfully tagged mzanur-awsdet-ecr:awsdet\n",
      "Login Succeeded\n",
      "The push refers to repository [578276202366.dkr.ecr.us-east-1.amazonaws.com/mzanur-awsdet-ecr]\n",
      "3421162ebb6e: Preparing\n",
      "99fa26134099: Preparing\n",
      "652a3bca3e7c: Preparing\n",
      "a642001beced: Preparing\n",
      "fb7e729f01ee: Preparing\n",
      "5f30d1abda20: Preparing\n",
      "62d0cea2f839: Preparing\n",
      "eb0041e686be: Preparing\n",
      "e5ad83f6ed12: Preparing\n",
      "c9c2998bcd26: Preparing\n",
      "715581c246d8: Preparing\n",
      "215350bbd6e2: Preparing\n",
      "39c954b40b83: Preparing\n",
      "0c2bae338b79: Preparing\n",
      "9437ba131cd0: Preparing\n",
      "ddc5013c0d50: Preparing\n",
      "f4471e48e4d7: Preparing\n",
      "b9fcae229fb0: Preparing\n",
      "405fd4c97e3f: Preparing\n",
      "808fd332a58a: Preparing\n",
      "e5ad83f6ed12: Waiting\n",
      "b16af11cbf29: Preparing\n",
      "37b9a4b22186: Preparing\n",
      "eb0041e686be: Waiting\n",
      "405fd4c97e3f: Waiting\n",
      "39c954b40b83: Waiting\n",
      "0c2bae338b79: Waiting\n",
      "808fd332a58a: Waiting\n",
      "b9fcae229fb0: Waiting\n",
      "b16af11cbf29: Waiting\n",
      "62d0cea2f839: Waiting\n",
      "e0b3afb09dc3: Preparing\n",
      "6c01b5a53aac: Preparing\n",
      "9437ba131cd0: Waiting\n",
      "37b9a4b22186: Waiting\n",
      "e0b3afb09dc3: Waiting\n",
      "215350bbd6e2: Waiting\n",
      "5f30d1abda20: Waiting\n",
      "715581c246d8: Waiting\n",
      "2c6ac8e5063e: Preparing\n",
      "cc967c529ced: Preparing\n",
      "2c6ac8e5063e: Waiting\n",
      "6c01b5a53aac: Waiting\n",
      "fb7e729f01ee: Layer already exists\n",
      "99fa26134099: Layer already exists\n",
      "3421162ebb6e: Layer already exists\n",
      "a642001beced: Layer already exists\n",
      "652a3bca3e7c: Layer already exists\n",
      "62d0cea2f839: Layer already exists\n",
      "5f30d1abda20: Layer already exists\n",
      "e5ad83f6ed12: Layer already exists\n",
      "eb0041e686be: Layer already exists\n",
      "c9c2998bcd26: Layer already exists\n",
      "715581c246d8: Layer already exists\n",
      "215350bbd6e2: Layer already exists\n",
      "39c954b40b83: Layer already exists\n",
      "ddc5013c0d50: Layer already exists\n",
      "0c2bae338b79: Layer already exists\n",
      "9437ba131cd0: Layer already exists\n",
      "808fd332a58a: Layer already exists\n",
      "f4471e48e4d7: Layer already exists\n",
      "b16af11cbf29: Layer already exists\n",
      "b9fcae229fb0: Layer already exists\n",
      "405fd4c97e3f: Layer already exists\n",
      "e0b3afb09dc3: Layer already exists\n",
      "37b9a4b22186: Layer already exists\n",
      "6c01b5a53aac: Layer already exists\n",
      "2c6ac8e5063e: Layer already exists\n",
      "cc967c529ced: Layer already exists\n",
      "awsdet: digest: sha256:4f4e6542da68962d303e4943ab07a88517ddd06fdbf4e42458f83d83a1f22f40 size: 5765\n",
      "578276202366.dkr.ecr.us-east-1.amazonaws.com/mzanur-awsdet-ecr:awsdet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/SageMaker/deep-learning-models/models/vision/detection/\n",
    "# IMPORTANT replace argument to the script with name of the ECR repository that you created\n",
    "scripts/build_docker_sagemaker.sh mzanur-awsdet-ecr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of running the script above it will emit your docker image ID. Please put in that image ID in the configuration for your desired model under `deep-learning-models/models/vision/detection/configs/<model>/SM/<configuration file>` as described in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker and Model configuration\n",
    "\n",
    "Now we can start setting up the model training process on SageMaker.\n",
    "\n",
    "Most of the hyperparameters related to the model structure do not require tweaking. Some training related hyperparameters, e.g. learning rates, momentum, weight decay may need changing depending on how many nodes you are using to train, or your dataset, or the batch size per GPU that you decide to choose.\n",
    "\n",
    "#### Brief explanation of parameters in configuration file\n",
    "\n",
    "`user_id` can be anything you like, used for keeping track of your training jobs\n",
    "\n",
    "`s3_bucket` name of your s3 bucket without s3://, e.g. jbsnyder-sagemaker\n",
    "\n",
    "`docker_image` - ID of the docker image id that was built, e.g. 12345.dkr.ecr.us-east-1.amazonaws.com/name:awsdet\n",
    "\n",
    "`instance_count` number of instances (nodes) for training.\n",
    "\n",
    "`instance_type` the type of EC2 instances that will be used for training. For highest performance use `ml.p3dn.24xlarge`. The `ml.p3.16xlarge` and `ml.p3.8xlarge` are slightly lower performance while being more cost effective. The `g4dn.12xlarge` instance type is even lower cost, but much slower than the p3 instances.\n",
    "\n",
    "`num_workers_per_host` number of training jobs per instance. This should be the number of GPUs on an instance, so should be set to 8 for `ml.p3dn.24xlarge` or `ml.p3.16xlarge` instances, and 4 for `ml.p3.8xlarge` or `g4dn.12xlarge`.\n",
    "\n",
    "`amp_enabled` refers to whether a model should train using mixed precision. Mixed precision training is supported on p3 and g4 instances.\n",
    "\n",
    "`warmup_steps` number of steps for which the learning rate will be increased from `learning_rate * warmup_ratio` to `learning_rate`.\n",
    "\n",
    "`optimizer` this is the configuration for an `tf.keras.optimizers` optimizer. The `learning_rate` specified here is the learning rate adjusted for 8 GPUs after warmup steps. If you have more than 8 GPUs then this rate will be scaled linearly automatically.\n",
    "\n",
    "`imgs_per_gpu` number of images per GPU.\n",
    "\n",
    "Other model specific details can be found and tweaked in the relevant code under `deep-learning-models/models/vision/detection/awsdet` directory. This is suggested for advanced users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Job Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/deep-learning-models/models/vision/detection\n",
      "Launched SageMaker job: mzanur-frcnn-24-06-2020-03-48\n"
     ]
    }
   ],
   "source": [
    "%cd ~/SageMaker/deep-learning-models/models/vision/detection/\n",
    "%run tools/launch_sagemaker_job.py --configuration configs/faster_rcnn/SM/faster_rcnn_r50_fpn_1x_coco.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the Sagemaker homepage and click `Training Jobs`, you should see a new training job with a name that was setup in the configuration file. Click the link to see what your training job is doing.\n",
    "\n",
    "The training takes about 5-10 minutes to set up, depending on the instance type that was chosen.\n",
    "\n",
    "The sagemaker job launch script downloads data required to train from S3, then the docker image (that was built) is downloaded from ECR. \n",
    "\n",
    "You can monitor CPU/GPU usage and see CloudWatch logs for each instance under the section called `Monitor`.\n",
    "\n",
    "For a more detailed view into training losses, accuracy, and to visualize predictions, e.g. bounding boxes, Tensorboard can be used. Once training starts, a new set of directories in your S3 bucket are created \n",
    "\n",
    "```\n",
    "[s3_path]/tensorboard/\n",
    "```\n",
    "\n",
    "In a terminal on your notebook instance, run\n",
    "\n",
    "```\n",
    "conda activate tensorflow_p36\n",
    "tensorboard --logdir [s3_path]/tensorboard\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
